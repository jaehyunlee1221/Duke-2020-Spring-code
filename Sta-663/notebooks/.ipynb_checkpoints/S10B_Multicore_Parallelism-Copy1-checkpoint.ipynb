{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Core Parallelism\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Value, Array\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla Python\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_pi(n):\n",
    "    s = 0\n",
    "    for i in range(n):\n",
    "        x = np.random.uniform(-1, 1)\n",
    "        y = np.random.uniform(-1, 1)\n",
    "        if (x**2 + y**2) < 1:\n",
    "            s += 1\n",
    "    return 4*s/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 584 ms, sys: 4 ms, total: 588 ms\n",
      "Wall time: 587 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.14364"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mc_pi(int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.02 s, sys: 8 ms, total: 6.03 s\n",
      "Wall time: 6.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "res = [mc_pi(int(1e5)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `concurrent.futures` module\n",
    "----\n",
    "\n",
    "Concurrent processes are processes that will return the same results regardless of the order in which they were executed. A \"future\" is something that will return a result sometime in the future.  The `concurrent.futures` module provides an event handler, which can be fed functions to be scheduled for future execution. This provides us with a simple model for parallel execution on a multi-core machine.\n",
    "\n",
    "While concurrent futures provide a simpler interface, it is slower and less flexible when compared with using `multiprocessing` for parallel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using processes in parallel with `ProcessPoolExecutor`\n",
    "----\n",
    "\n",
    "We get a linear speedup as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 36 ms, total: 68 ms\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e5) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.14444, 3.14444, 3.14444, 3.14444, 3.14568, 3.14568, 3.14568,\n",
       "       3.14568, 3.14292, 3.14292])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you have many jobs\n",
    "\n",
    "The `futures` object gives fine control over the process, such as adding callbacks and canceling a submitted job, but is computationally expensive. We can use the `chunksize` argument to reduce this cost when submitting many jobs - this specifies the number of tasks to be given to a worker at a time. A detailed explanation of `chunksize` is provided [here](https://stackoverflow.com/questions/53751050/python-multiprocessing-understanding-logic-behind-chunksize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using default `chunksize `\n",
    "\n",
    "The total amount of computation whether you have 10 jobs of size 10,000,000 or 10,000 jobs of size 10,000 is essentially the same, so we would expect them both to take about the same amount of time, but this is not true due to the overhead described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.59 s, sys: 1.96 s, total: 5.55 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e2) for i in range(int(1e4))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `chunksize` of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 ms, sys: 56 ms, total: 108 ms\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e2) for i in range(int(1e4))], chunksize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine control of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status of processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x**2\n",
    "\n",
    "def f2(x, y):\n",
    "    return x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a running: True\n",
      "a done: False\n",
      "b running: False\n",
      "b done: False\n",
      "c running: False\n",
      "c done: False\n",
      "a result 1\n",
      "b result 2\n",
      "c result 100\n"
     ]
    }
   ],
   "source": [
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    a = pool.submit(f2, 1, 1)\n",
    "    b = pool.submit(f2, 1,2)\n",
    "    c = pool.submit(f1, 10)    \n",
    "\n",
    "    print('a running:', a.running())\n",
    "    print('a done:', a.done())\n",
    "\n",
    "    print('b running:', b.running())\n",
    "    print('b done:', b.done())\n",
    "\n",
    "    print('c running:', c.running())\n",
    "    print('c done:', c.done())\n",
    "\n",
    "    print('a result', a.result())\n",
    "    print('b result', b.result())\n",
    "    print('c result', c.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canceling jobs and adding callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process done!\n",
      "Process done!\n",
      "Process done!\n",
      "Process done!\n",
      "Process done!\n",
      "0.31519074298915756Process done!\n",
      "\n",
      "0.06704854566590644\n",
      "0.1487331123666466\n",
      "Process done!\n",
      "0.31240894356208665\n",
      "0.08606349503792518\n",
      "0.042283915266939115\n",
      "0.1251136196055298\n",
      "0.3633936992188428\n",
      "0.35568598614576685\n",
      "0.673662927280906\n",
      "0.9454890321275123\n",
      "0.022222962987650002Process done!\n",
      "0.008743570535942578\n",
      "0.3442006639040779\n",
      "\n",
      "0.11265572032056531\n",
      "Process done!\n",
      "0.024921543903694235\n",
      "0.20528728247554254\n",
      "Running\n",
      "Process done!\n",
      "Process done!\n",
      "Process done!\n",
      "0.11719654784926778\n",
      "0.08696919374340895\n",
      "0.42954575400997613\n",
      "0.28141424648261726\n",
      "0.05920277319195527\n",
      "0.19906252971138028\n",
      "0.3350965655000501\n"
     ]
    }
   ],
   "source": [
    "njobs = 24\n",
    "\n",
    "res = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "\n",
    "    for i in range(njobs):\n",
    "        res.append(pool.submit(f2, *np.random.rand(2)))\n",
    "        if i % 2 == 0:\n",
    "            res[i].add_done_callback(lambda future: print(\"Process done!\"))\n",
    "    res[4].cancel()\n",
    "    if res[4].cancelled():\n",
    "        print(\"Process 4 cancelled\")\n",
    "\n",
    "    for i, x in enumerate(res):\n",
    "        while x.running():\n",
    "            print(\"Running\")\n",
    "            time.sleep(1)\n",
    "        if not x.cancelled():\n",
    "            print(x.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions with multiple arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a function adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_(args):\n",
    "    return f(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(24)\n",
    "chunks = np.array_split(xs, xs.shape[0]//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1]),\n",
       " array([2, 3]),\n",
       " array([4, 5]),\n",
       " array([6, 7]),\n",
       " array([8, 9]),\n",
       " array([10, 11]),\n",
       " array([12, 13]),\n",
       " array([14, 15]),\n",
       " array([16, 17]),\n",
       " array([18, 19]),\n",
       " array([20, 21]),\n",
       " array([22, 23])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(f_, chunks)\n",
    "list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using processes in parallel with ThreadPoolExecutor\n",
    "----\n",
    "\n",
    "We do not get any speedup because the GIL only allows one thread to run at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.36 s, sys: 0 ns, total: 6.36 s\n",
      "Wall time: 6.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e5) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.14232, 3.13844, 3.14356, 3.13676, 3.13676, 3.13928, 3.13996,\n",
       "       3.1428 , 3.14516, 3.14312])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `multiprocessing`\n",
    "\n",
    "The `concurrent.futures.ProcessPoolExecutor` is actually a wrapper for `multiprocessing.Pool` to unify the threading and process interfaces. I typically just work directly with `mutliprocessing` since I don't have much use for threads. One nice thing about using `multiprocessing` apart from more fine-grai control if you need it, is that it works equally well for small numbers of large jobs, or large numbers of small jobs out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 0 ns, total: 28 ms\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e5) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.13924, 3.13924, 3.13924, 3.13924, 3.13504, 3.13504, 3.13504,\n",
       "       3.13504, 3.14184, 3.14184])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 24 ms, total: 40 ms\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e2) for i in range(int(1e4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.12, 3.04, 3.24, ..., 3.12, 3.36, 3.12])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions with multiple arguments\n",
    "\n",
    "Multiprocessing `Pool` has a `starmap` method that removes the need to write a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.arange(24)\n",
    "with Pool(processes=4) as pool:\n",
    "    res = pool.starmap(f, np.array_split(xs, xs.shape[0]//2))\n",
    "list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial application\n",
    "\n",
    "Sometimes, `functools.partial` can be used to reduce the number of arguments needed to just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "fp = partial(f, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32,\n",
       "       34, 36, 38, 40, 42, 44, 46])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.arange(24)\n",
    "with Pool(processes=4) as pool:\n",
    "    res = pool.map(fp, xs)\n",
    "np.array(list(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking and non-blocking calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(n):\n",
    "    time.sleep(n)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control back!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Pool(processes=4) as pool:\n",
    "    res = pool.map(func, [3,3,3,3,3])\n",
    "    print(\"Control back!\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control back!\n",
      "False\n",
      "True\n",
      "[3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=4) as pool:\n",
    "    res = pool.map_async(func, [3,3,3,3,3])\n",
    "    print(\"Control back!\")\n",
    "    print(res.ready())\n",
    "    res.wait()\n",
    "    print(res.ready())\n",
    "    print(res.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different jobs to different processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(n):\n",
    "    time.sleep(1)\n",
    "    return n\n",
    "\n",
    "def f2(n):\n",
    "    time.sleep(1)\n",
    "    return n**2\n",
    "\n",
    "def f3(n):\n",
    "    time.sleep(1)\n",
    "    return n**3\n",
    "\n",
    "def f4(n):\n",
    "    time.sleep(1)\n",
    "    return n**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 16), (1, 4), (2, 8), (3, 2)]\n",
      "CPU times: user 4 ms, sys: 40 ms, total: 44 ms\n",
      "Wall time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "    res = []\n",
    "    for i, f in enumerate([f4, f2, f3, f1]):\n",
    "        res.append((i, pool.apply(f, [2])))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 16), (1, 4), (2, 8), (3, 2)]\n",
      "CPU times: user 20 ms, sys: 20 ms, total: 40 ms\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "    res = []\n",
    "    for i, f in enumerate([f4, f2, f3, f1]):\n",
    "        res.append((i, pool.apply_async(f, [2])))\n",
    "    print([(i, r.get()) for i, r in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating individual processes\n",
    "\n",
    "If you need more control over individual processes than Pool provides - namely, if you need to share information across processes, you can work with individual workers and thread-safe memory structures. This is just for completeness as most data processing tasks do not require this level of control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(i):\n",
    "    time.sleep(np.random.random())\n",
    "    print(os.getpid(), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11011 0\n",
      "11014 1\n",
      "11017 2\n",
      "11020 3\n",
      "11023 4\n",
      "11026 5\n",
      "11029 6\n",
      "11032 7\n",
      "11035 8\n",
      "11038 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    p = mp.Process(target=f, args=(i,))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Queues to share information between processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(q, i):\n",
    "    time.sleep(np.random.random())\n",
    "    q.put((os.getpid(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11041, 0),\n",
       " (11043, 1),\n",
       " (11045, 2),\n",
       " (11047, 3),\n",
       " (11049, 4),\n",
       " (11051, 5),\n",
       " (11053, 6),\n",
       " (11055, 7),\n",
       " (11057, 8),\n",
       " (11059, 9)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = mp.Queue()\n",
    "\n",
    "res = []\n",
    "for i in range(10):\n",
    "    p = mp.Process(target=f1, args=(q,i,))\n",
    "    p.start()\n",
    "    res.append(q.get())\n",
    "    p.join()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Value and Array for sharing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of jobs (1)\n",
    "\n",
    "This does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(i):\n",
    "    global counter\n",
    "    counter = counter + 1\n",
    "    print(os.getpid(), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10887 10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "f2(10)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11061 0\n",
      "11064 1\n",
      "11067 2\n",
      "11070 3\n",
      "11073 4\n",
      "11076 5\n",
      "11079 6\n",
      "11082 7\n",
      "11085 8\n",
      "11088 9\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in range(10):\n",
    "    p = mp.Process(target=f2, args=(i,))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that separate processes have their own memory and DO NOT share global memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of jobs (2)\n",
    "\n",
    "We can use shared memory to do this, but it is slow because multiprocessing has to ensure that only one process gets to use counter at any one time. Multiprocesing provides Value and Array shared memory variables, but you can also convert arbitrary Python variables into shared memory objects (less efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(i, counter, store):\n",
    "    counter.value += 1\n",
    "    store[os.getpid() % 10] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "CPU times: user 68 ms, sys: 240 ms, total: 308 ms\n",
      "Wall time: 745 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "counter = mp.Value('i', 0)\n",
    "store = mp.Array('i', [0]*10)\n",
    "\n",
    "for i in range(int(1e2)):\n",
    "    p = mp.Process(target=f3, args=(i, counter, store))\n",
    "    p.start()\n",
    "    p.join()\n",
    "\n",
    "print(counter.value)\n",
    "print(store[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding use of shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of jobs (3)\n",
    "\n",
    "We should try to avoid using shared memory as much as possible in parallel jobs as they drastically reduce efficiency. One useful approach is to use the `map-reduce` pattern. We should also use Pool to reuse processes rather than spawn too many of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(i):\n",
    "    return (os.getpid(), 1, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 24 ms, total: 28 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# map step\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    res = pool.map(f4, range(int(1e2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11192,     1,    53],\n",
       "       [11191,     1,    69],\n",
       "       [11193,     1,    17],\n",
       "       [11191,     1,    32],\n",
       "       [11193,     1,    14],\n",
       "       [11191,     1,    63],\n",
       "       [11192,     1,    36],\n",
       "       [11191,     1,    44],\n",
       "       [11191,     1,    48],\n",
       "       [11191,     1,    79]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[np.random.choice(len(res), 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=['pid', 'one', 'i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11191</th>\n",
       "      <td>35</td>\n",
       "      <td>1575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11192</th>\n",
       "      <td>28</td>\n",
       "      <td>1358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11193</th>\n",
       "      <td>28</td>\n",
       "      <td>1652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11194</th>\n",
       "      <td>9</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       one     i\n",
       "pid             \n",
       "11191   35  1575\n",
       "11192   28  1358\n",
       "11193   28  1652\n",
       "11194    9   365"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('pid').sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
